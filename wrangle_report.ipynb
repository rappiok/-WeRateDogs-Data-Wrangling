{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reporting: WeRateDogs wragle_report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "This project is about wrangling of tweet archive of Twitter user @dog_rates, also known as WeRateDogs. WeRateDogs is a Twitter account that rates people's dogs with a humorous comment about the dog. There are 3 main dataset that is going to be used, and these are :\n",
    "\n",
    "* twitter_archive_enhanced.csv : it contains all the archive of the twitter about dog rating.\n",
    "* image_predictions.tsv : present in each tweet according to a neural network.\n",
    "* tweet_json : contains each tweet's retweet count and favorite (\"like\") count at the minimum and any additional data you find interesting.\n",
    "\n",
    "Project Steps Overview\n",
    "\n",
    " 1. Gathering data\n",
    " 2. Assessing data\n",
    " 3. Cleaning data\n",
    " 4. Storing data\n",
    " 5. Analyzing, and visualizing data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gathering data\n",
    "The three dataset were gathered through different means which are: \n",
    "1. Directly downloading the WeRateDogs Twitter archive data (file on hand, manual download of 'twitter-archiveenhanced.csv')\n",
    "2. Programmatically downloading the tweet image prediction using the **Requests** library (image_predictions.tsv)\n",
    "3. Was suppose to use the tweepy library to query the additional data via the Twitter API (tweet_json.txt) but was haing issues with the developers privilege, so downloaded the one Udacity provided using the **Requests** library and reading this file line by line, to create a pandas DataFrame that you will soon assess and clean. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assessing data\n",
    "Assesed the dataset using both visual and programmatic assessement process to get a clear overview of the data and be able to determine the quality and tidiness of the data.\n",
    "\n",
    "> Visual Assessment\n",
    "    First the datasets were opened in an external application (Excel) to have an overview and was also displayed in the notebook. \n",
    " \n",
    "> Programmatic Assessement\n",
    "    Pandasâ€™ functions and/or methods such as infor, describe, isnull, duplicate, dtypes, sample, etc were used to assess the data.\n",
    "\n",
    "* After assessing the various datasets the following issues were identified. \n",
    "\n",
    "> Quality issues\n",
    "    1. Null values recorded as None instead of NaN\n",
    "    2. Some columns in the twitter_ar dataset holds no data or very little amount of data\n",
    "    3. expanded_urls in the twitter_ar dataset has missing values\n",
    "    4. Same data type of the twitter_ar dataset columns are not right\n",
    "    5. Invalid dog names in the twitter_ar\n",
    "    6. Columns p1, p2 and p3 in the image_pred dataset have values as upper case sometimes lowercase other times\n",
    "    7. tweet id title in the tweet_df dataset has a different naming to the id column name for other datasets.\n",
    "    8. In the image_pred dataset there are image duplicate predictions \n",
    "    9. In the image_pred dataset, the column names are not clearly descriptive\n",
    "    10. In the twitter_ar dataset, rating_denominator is other than assumed standard value of 10 at some places which can be Inaccurate data\n",
    "    11. In the twitter_ar dataset, rating_numerator column has some exceptionally high values which leads to exceptionally high rating which can be inaccurate\n",
    "\n",
    "> Tidiness issues\n",
    "    1. Redundent columns of same category (doggo, floofer, pupper, puppo), must be made one column\n",
    "    2.Merge all three dataset image_pred, twitter_ar and tweet_df\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning data\n",
    "In this section, the focus is to clean all the issues detected in the data assessment level.\n",
    "* The first step was to create copies of the original datasets that were gathered.\n",
    "* After all the issues identified at the data assessment level were resloved; quality issues tackled first, then tidiness\n",
    "\n",
    "> Quality issues cleaning :\n",
    "    1. Check and replace all 'none' records in the twitter_clean dataset, particularly for doggo, floofer, pupper, and puppo columns\n",
    "    2. check and drop the unnecessary columns in_reply_to_status_id, in_reply_to_user_id, retweeted_status_timestamp and retweeted_status_user_id\n",
    "    3. Check and fill the missing values of expanded_urls column in tweet_data_clean table using tweet_id as tweet id is the last part of the tweet URL after \"status/\"\n",
    "    4. Change the datatype of timestamp to datetime\n",
    "    5. Check and replace all the names of dogs having invalid data with NaN\n",
    "    6. Check and replace the instance of _ with space and change the values to upper case in p1, p2 and p3\n",
    "    7. Check and rename the id column to tweet_id for tweet_clean\n",
    "    8. Check and drop rows where image_url is duplicated\n",
    "    9. Rename the columns with descriptive names\n",
    "    10. Replace the doggo pupper, doggofloofer and doggo puppo with multiple\n",
    "    11. Check for actual rating in the text column and replace all values in rating_numerator column that has some exceptionally high values in the twitter_ar dataset, 
    "    \n",
    "    \n",
    "> Tidiness issues cleaning:\n",
    "    1. Make the doggo, floofer, pupper, puppo columns one column that is dog_status and after drop the doggo, floofer, pupper, puppo columns.\n",
    "    2. Dropping the redundant columns\n",
    "    3. Merge the twitter_clean and tweet_clean dataset on tweet_id column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Storing data\n",
    "Saved gathered, assessed, and cleaned master dataset to a CSV file named \"twitter_archive_master.csv\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyzing, and visualizing data\n",
    "The questions we would like to get insight of include the following:\n",
    "1. How often or active was the weratedog page \n",
    "2. How popular are the various dog stage\n",
    "3. Are the ratings by WeRateDogs relative to the retweet count and favorite count?\n",
    "4. correlation between ratings by WeRateDogs relative to the retweet count and favorite count?\n",
    "\n",
    "Ways of answering the questions asked\n",
    "1. Extract the data that would be used to answer this question and find the year of tweet from the timestamp, and analyse based on a pie visual\n",
    "2. Use extracted data to answer this question and find the umber of tweet per dog stage and visualize based on a bar graph\n",
    "3. Extract the data that would be used to answer this questionof the relation between rating_numerator, and retweet_count and favorite count, then visualize based on a scatter plot\n",
    "4. Use the extract the data that would be used to answer this question of the correlation between rating_numerator, and retweet_count and favorite count, then visualize based on a scatter plot and calculating the correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
